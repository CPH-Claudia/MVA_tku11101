# load data
data <- read.table("C:/Users/user/Desktop/多變量11101/uscrime.dat")
x <- data[,(3:9)]
# define variable names
colnames(x) = c("murder", "rape", "robbery", "assault", "burglary", "larcery", "autothieft")
View(x)
row.s    = apply(x, 1, sum)        # row sums
column.s = apply(x, 2, sum)        # column sums
mat.s    = sum(x)                  # matrix sum
D        = matrix(0, nrow = 50, ncol = 50)
View(D)
# distance for rows
for (i in 1:(dim(x)[1] - 1)) {
for (j in (i + 1):dim(x)[1]) {
for (z in 1:dim(x)[2]) {
D[i, j] = 1/(column.s[z]/mat.s) * ((x[i, z]/row.s[i]) - (x[j, z]/row.s[j]))^2
}
}
}
D
View(D)
weightD <- scale(D)                      # standardize the data
View(weightD)
sx <- scale(x)                      # standardize the data
row.s    = apply(x, 1, sum)        # row sums
column.s = apply(x, 2, sum)        # column sums
mat.s    = sum(x)                  # matrix sum
D        = matrix(0, nrow = 50, ncol = 50)
# distance for rows
for (i in 1:(dim(x)[1] - 1)) {
for (j in (i + 1):dim(x)[1]) {
for (z in 1:dim(x)[2]) {
D[i, j] = 1/(column.s[z]/mat.s) * ((x[i, z]/row.s[i]) - (x[j, z]/row.s[j]))^2
}
}
}
D
View(D)
# k-means clustering algorithm
library(ClusterR)
# k-means clustering algorithm
install.packages("ClusterR")
library(ClusterR)
library(cluster)
set.seed(1221)
results <- kmeans(x, 4, algorithm="Lloyd")
results
results$centers
plot(x, type="n", xlab="price conciousness",ylab="brand loyalty",main="k-means clustering", xlim=c(-4,4))
plot(x, type="l", xlab="price conciousness",ylab="brand loyalty",main="k-means clustering", xlim=c(-4,4))
plot(x, xlab="price conciousness",ylab="brand loyalty",main="k-means clustering", xlim=c(-4,4))
plot(x, main="k-means clustering", xlim=c(-4,4))
plot(sx, main="k-means clustering", xlim=c(-4,4))
plot(x, main="k-means clustering", xlim=c(-4,4))
plot(x, main="k-means clustering")
points(results$centers[1,1],results$centers[1,2], col = "black")
points(results$centers[1,1],results$centers[1,2],results$centers[1,3],results$centers[1,4],results$centers[1,5],results$centers[1,6],results$centers[1,7], col = "black")
results <- kmeans(D, 4, algorithm="Lloyd")
results
# correlation coefficient
round(cor(x), digits = 2)
library(ggplot2)
library(GGally)
ggpairs(data = x, mapping = aes(color = type, alpha = 0.5))
# spectral decomposition
eig = eigen(cov(x1))
# x <- transform(x, rbind(x[1:9, ] == "EN", x[10:21, ] == "MW", x[22:37, ] == "SOU", x[38:50, ] == "WES"))
x1 <- scale(x)  # standardize variable
# spectral decomposition
eig = eigen(cov(x1))
e = eig$values
v = eig$vectors[, 1:2]
dav = x1 %*% v
# 自定義點的形狀&顏色
tree[tree.wd == 1] = 1
tree[tree.wd == 2] = 2
tree.wd[tree.wd == 1] = 1
tree.wd[tree.wd == 2] = 2
tree.wd[tree.wd == 3] = 3
tree.wd[tree.wd == 4] = 4
tree.wd = cutree(w, 4)
# Ward algorithm
d <- dist(x1, "euclidean", p = 2)   # euclidean distance matrix(p=power of Minkowski distance)
dd <- d^2
w <- hclust(dd, method = "ward.D")  # cluster analysis with ward algorithm
tree.wd = cutree(w, 4)
#plot results of ward algorithm
fviz_cluster(list(data = x1, cluster = tree.wd))
t1   = subset(x1, tree.wd == 1)
t2   = subset(x1, tree.wd == 2)
t3   = subset(x1, tree.wd == 3)
t4   = subset(x1, tree.wd == 4)
# Plot 1: Dendrogram for the standardized data after Ward
plot(w, hang = -0.1, labels = FALSE, frame.plot = TRUE, ann = FALSE)
title(main = "Ward Dendrogram", ylab = "distance")
rect.hclust(w,k=4) # k=4的分類線
# means for 4 Clusters
mc.wd <- aggregate(x1, by=list(cluster=tree.wd), mean)
# standard deviations for 4 Clusters
sc.wd <- aggregate(x1, by=list(cluster=tree.wd), sd)
# spectral decomposition
eig = eigen(cov(x1))
e = eig$values
v = eig$vectors[, 1:2]
dav = x1 %*% v
#
cum  = cumsum(e)/sum(e)
tree.wd[tree.wd == 1] = 1
tree.wd[tree.wd == 2] = 2
tree.wd[tree.wd == 3] = 3
tree.wd[tree.wd == 4] = 4
tr = tree.wd
tr[tr == 1] = "red"
tr[tr == 2] = "black"
tr[tr == 3] = "blue"
tr[tr == 4] = "orange"
# Plot 2: Scatterplot for the first two PCs displaying the two clusters
dev.new()
plot(dav[, 1], dav[, 2], pch = tree.wd, col = tr, xlab = "PC1", ylab = "PC2", main = "first vs. second PC")
# c.f.
# correlation coefficient
r <- round(cor(x), digits = 3)
library(ggplot2)
library(GGally)
ggpairs(data = x, alpha = 0.5)
table <- cbind(x1, tree.wd, k.means$cluster)
k.means <- kmeans(x1, 4, nstart = 25)
table <- cbind(x1, tree.wd, k.means$cluster)
View(table)
k.means$cluster
k.means
mc  = cbind(colMeans(subset(x1, tree == "1")), colMeans(subset(x1, tree == "2")),
colMeans(subset(x1, tree == "3")), colMeans(subset(x1, tree == "4")))
mc  = cbind(colMeans(subset(x1, by=list(cluster=k.means$cluster))))
View(mc)
list(cluster=k.means$cluster)
mc  = cbind(colMeans(subset(da, k.means$cluster == "1")), colMeans(subset(da, k.means$cluster == "2")),
colMeans(subset(da, k.means$cluster == "3")), colMeans(subset(da, k.means$cluster == "4")))
mc  = cbind(colMeans(subset(x1, k.means$cluster == "1")), colMeans(subset(x1, k.means$cluster == "2")),
colMeans(subset(x1, k.means$cluster == "3")), colMeans(subset(x1, k.means$cluster == "4")))
View(mc)
View(t1)
k.means$cluster == "1"
subset(x1, k.means$cluster == "1")
sc.km <- cbind(sd(subset(x1, k.means$cluster == "1")[, 1:ncol(x1)]),
sd(subset(x1, k.means$cluster == "2")[, 1:ncol(x1)]),
sd(subset(x1, k.means$cluster == "3")[, 1:ncol(x1)]),
sd(subset(x1, k.means$cluster == "4")[, 1:ncol(x1)]))
View(sc.km)
# means for 4 Clusters
mc.wd <- cbind(colMeans(subset(x1, tree == "1")), colMeans(subset(x1, tree == "2")),
colMeans(subset(x1, tree == "3")), colMeans(subset(x1, tree == "4")))
# means for 4 Clusters
mc.wd <- cbind(colMeans(subset(x1, tree.wd == "1")), colMeans(subset(x1, tree.wd == "2")),
colMeans(subset(x1, tree.wd == "3")), colMeans(subset(x1, tree.wd == "4")))
# standard deviations for 4 Clusters
sc.wd <- cbind(sd(t1[, 1:ncol(x1)]), sd(t2[, 1:ncol(x1)]),
sd(t3[, 1:ncol(x1)]), sd(t4[, 1:ncol(x1)]))
View(sc.wd)
# means and standard deviations of the standardized variables for Cluster 1
tbl.wd = cbind(mc.wd[, 1], sc.wd[, 1]/sqrt(nrow(t1)), mc.wd[, 2], sc.wd[, 2]/sqrt(nrow(t2)),
mc.wd[, 3], sc.wd[, 3]/sqrt(nrow(t3)), mc.wd[, 4], sc.wd[, 4]/sqrt(nrow(t4)))
View(tbl.wd)
# clear all variables
rm(list = ls(all = TRUE))
graphics.off()
# load data
data = read.table("bostonh.dat")
# load data
data = read.table("C:/Users/user/Desktop/多變量11101/MVA-ToDo-master/QID-1204-MVAclusbh/bostonh.dat")
# transform data
xt       = data
xt[, 1]  = log(data[, 1])
xt[, 2]  = data[, 2]/10
xt[, 3]  = log(data[, 3])
xt[, 5]  = log(data[, 5])
xt[, 6]  = log(data[, 6])
xt[, 7]  = (data[, 7]^(2.5))/10000
xt[, 8]  = log(data[, 8])
xt[, 9]  = log(data[, 9])
xt[, 10] = log(data[, 10])
xt[, 11] = exp(0.4 * data[, 11])/1000
xt[, 12] = data[, 12]/100
xt[, 13] = sqrt(data[, 13])
xt[, 14] = log(data[, 14])
data     = xt[, -4]
da   = scale(data)                    # standardize variables
d    = dist(da, "euclidean", p = 2)   # euclidean distance matrix
w    = hclust(d, method = "ward.D")   # cluster analysis with ward algorithm
tree = cutree(w, 2)
t1   = subset(da, tree == 1)
t2   = subset(da, tree == 2)
# Plot 1: Dendrogram for the standardized food.dat after Ward algorithm
plot(w, hang = -0.1, labels = FALSE, frame.plot = TRUE, ann = FALSE, cex.axis = 1.2)
title(main = "Ward method", ylab = "distance", cex.lab = 1.2, cex.main = 1.6)
# means for Cluster 1 and Cluster 2
mc  = cbind(colMeans(subset(da, tree == "1")), colMeans(subset(da, tree == "2")))
# standard deviations for Cluster 1 and Cluster 2
sc  = cbind(sd(t1[, 1:ncol(da)]), sd(t2[, 1:ncol(da)]))
# means and standard deviations of the 13 standardized variables for Cluster 1
# (251 observations) and Cluster 2 (255 observations)
tbl = cbind(mc[, 1], sc[, 1]/sqrt(nrow(t1)), mc[, 2], sc[, 2]/sqrt(nrow(t2)))
View(sc)
View(tbl)
sc[, 1]/sqrt(nrow(t1))
mc[, 1]
sc[, 1]
sqrt(nrow(t1))
t1[, 1:ncol(da)]
sd(t1[, 1:ncol(da)])
colsd(t1[, 1:ncol(da)])
# means for Cluster 1 and Cluster 2
mc  = cbind(colMeans(t1), colMeans(subset(t1)))
View(mc)
# standard deviations for Cluster 1 and Cluster 2
sc  = cbind(colSds(t1[, 1:ncol(da)]), sd(t2[, 1:ncol(da)]))
# standard deviations for Cluster 1 and Cluster 2
library(matrixStats)
sc  = cbind(colSds(t1[, 1:ncol(da)]), sd(t2[, 1:ncol(da)]))
View(sc)
sc  = cbind(colSds(t1[, 1:ncol(da)]), colSds(t2[, 1:ncol(da)]))
View(sc)
# means for Cluster 1 and Cluster 2
mc  = cbind(colMeans(t1), colMeans(subset(t2)))
View(mc)
# means for Cluster 1 and Cluster 2
mc  = cbind(colMeans(t1), colMeans(t2))
View(mc)
sc  = cbind(colSds(subset(da, tree == "1")[, 1:ncol(da)]), colSds(subset(da, tree == "2")[, 1:ncol(da)]))
View(sc)
View(da)
da=scale(xt)
sc  = cbind(colSds(subset(da, tree == "1")[, 1:ncol(da)]), colSds(subset(da, tree == "2")[, 1:ncol(da)]))
View(sc)
sc  = cbind(colStdevs(subset(da, tree == "1")[, 1:ncol(da)]), colStdevs(subset(da, tree == "2")[, 1:ncol(da)]))
require('fBasics')
install.packages("fBasics")
require('fBasics')
sc  = cbind(colStdevs(subset(da, tree == "1")[, 1:ncol(da)]), colStdevs(subset(da, tree == "2")[, 1:ncol(da)]))
colSds(subset(da, tree == "1")[, 1:ncol(da)])
# clear variables and close windows
rm(list = ls(all = TRUE))
# load data
data <- read.table("C:/Users/user/Desktop/多變量11101/uscrime.dat")
x <- data[,(3:9)]
# x <- transform(x, rbind(x[1:9, ] == "EN", x[10:21, ] == "MW", x[22:37, ] == "SOU", x[38:50, ] == "WES"))
x1 <- scale(x)  # standardize variable
# define variable names
colnames(x1) = c("murder", "rape", "robbery", "assault", "burglary", "larcery", "autothieft")
# k-means clustering algorithm
# install.packages("ClusterR")
library(ClusterR)
library(cluster)
library(factoextra)
set.seed(1221)
k.means <- kmeans(x1, 4, nstart = 25)
#plot results of final k-means model
fviz_cluster(k.means, data = x1)
#find means of each cluster
mc.km  = cbind(colMeans(subset(x1, k.means$cluster == "1")), colMeans(subset(x1, k.means$cluster == "2")),
colMeans(subset(x1, k.means$cluster == "3")), colMeans(subset(x1, k.means$cluster == "4")))
library(matrixStats)
sc.km <- cbind(colSds(subset(x1, k.means$cluster == "1")[, 1:ncol(x1)]),
colSds(subset(x1, k.means$cluster == "2")[, 1:ncol(x1)]),
colSds(subset(x1, k.means$cluster == "3")[, 1:ncol(x1)]),
colSds(subset(x1, k.means$cluster == "4")[, 1:ncol(x1)]))
tbl.km <- cbind(mc.km[, 1], sc.km[, 1]/sqrt(nrow(k.means$cluster == "1")),
mc.km[, 2], sc.km[, 2]/sqrt(nrow(k.means$cluster == "2")),
mc.km[, 3], sc.km[, 3]/sqrt(nrow(k.means$cluster == "3")),
mc.km[, 4], sc.km[, 4]/sqrt(nrow(k.means$cluster == "4")))
tbl.km <- cbind(mc.km[, 1], sc.km[, 1]/sqrt(nrow(subset(x1, k.means$cluster == "1"))),
mc.km[, 2], sc.km[, 2]/sqrt(nrow(subset(x1, k.means$cluster == "2"))),
mc.km[, 3], sc.km[, 3]/sqrt(nrow(subset(x1, k.means$cluster == "3"))),
mc.km[, 4], sc.km[, 4]/sqrt(nrow(subset(x1, k.means$cluster == "4"))))
View(tbl.km)
k.means
cbind(x, rep("en", 9), rep("mw", 12), rep("sou", 16), rep("wes", 13))
View(x)
cbind(x, rep("en"=x[1:9,], "mw"=x[10:21,], "sou"=x[22:37,], "wes"=x[38:50,]))
View(tbl.km)
plot(w, hang = -0.1, labels = FALSE, frame.plot = TRUE, ann = FALSE)
title(main = "Ward Dendrogram", ylab = "distance")
rect.hclust(w,k=4) # k=4的分類線
set.seed(1221)
# Ward algorithm
d <- dist(x1, "euclidean", p = 2)   # euclidean distance matrix(p=power of Minkowski distance)
dd <- d^2
w <- hclust(dd, method = "ward.D")  # cluster analysis with ward algorithm
tree.wd = cutree(w, 4)
#plot results of ward algorithm
fviz_cluster(list(data = x1, cluster = tree.wd))
t1   = subset(x1, tree.wd == 1)
t2   = subset(x1, tree.wd == 2)
t3   = subset(x1, tree.wd == 3)
t4   = subset(x1, tree.wd == 4)
# Plot 1: Dendrogram for the standardized data after Ward
plot(w, hang = -0.1, labels = FALSE, frame.plot = TRUE, ann = FALSE)
title(main = "Ward Dendrogram", ylab = "distance")
rect.hclust(w,k=4) # k=4的分類線
# means for 4 Clusters
mc.wd <- cbind(colMeans(subset(x1, tree.wd == "1")), colMeans(subset(x1, tree.wd == "2")),
colMeans(subset(x1, tree.wd == "3")), colMeans(subset(x1, tree.wd == "4")))
# standard deviations for 4 Clusters
sc.wd <- cbind(colSds(t1[, 1:ncol(x1)]), colSds(t2[, 1:ncol(x1)]),
colSds(t3[, 1:ncol(x1)]), colSds(t4[, 1:ncol(x1)]))
# means and standard deviations of the standardized variables for 4 Clusters
tbl.wd = cbind(mc.wd[, 1], sc.wd[, 1]/sqrt(nrow(t1)), mc.wd[, 2], sc.wd[, 2]/sqrt(nrow(t2)),
mc.wd[, 3], sc.wd[, 3]/sqrt(nrow(t3)), mc.wd[, 4], sc.wd[, 4]/sqrt(nrow(t4)))
View(tbl.wd)
eig = eigen(cov(x1))
e = eig$values
v = eig$vectors[, 1:2]
dav = x1 %*% v
# c.f.
# correlation coefficient
r <- round(cor(x1), digits = 3)
View(r)
library(ggplot2)
library(GGally)
ggpairs(data = x1, alpha = 0.5)
ggpairs(data = as.data.frame(x1), alpha = 0.5)
tree.wd
table <- cbind(x1, tree.wd, k.means$cluster)
View(table)
colnames(table) <- c("murder", "rape", "robbery", "assault", "burglary",
"larcery", "autothieft", "ward", "kmeans")
View(table)
# pairs(x[, 1:7], col = tr, upper.panel = NULL, labels = c("murder", "rape", "robbery",
# "assault", "burglary", "larcery",
# "autothieft"), cex.axis = 1.2)
ggpairs(data = x, mapping = aes(color = tr), alpha = 0.5)
tree.wd[tree.wd == 1] = 1
tree.wd[tree.wd == 2] = 2
tree.wd[tree.wd == 3] = 3
tree.wd[tree.wd == 4] = 4
tr = tree.wd
tr[tr == 1] = "red"
tr[tr == 2] = "black"
tr[tr == 3] = "blue"
tr[tr == 4] = "orange"
# pairs(x[, 1:7], col = tr, upper.panel = NULL, labels = c("murder", "rape", "robbery",
# "assault", "burglary", "larcery",
# "autothieft"), cex.axis = 1.2)
ggpairs(data = x, mapping = aes(color = tr), alpha = 0.5)
row.s    = apply(x, 1, sum)        # row sums
column.s = apply(x, 2, sum)        # column sums
mat.s    = sum(x)                  # matrix sum
D        = matrix(0, nrow = 50, ncol = 50)
# distance for rows
for (i in 1:(dim(x)[1] - 1)) {
for (j in (i + 1):dim(x)[1]) {
for (z in 1:dim(x)[2]) {
D[i, j] = 1/(column.s[z]/mat.s) * ((x[i, z]/row.s[i]) - (x[j, z]/row.s[j]))^2
}
}
}
D
k.means$centers
k.means
# Ward algorithm
d <- dist(x1, "euclidean", p = 2)   # euclidean distance matrix(p=power of Minkowski distance)
dd <- d^2
w <- hclust(dd, method = "ward.D")  # cluster analysis with ward algorithm
tree.wd = cutree(w, 4)
# plot results of ward algorithm
fviz_cluster(list(data = x1, cluster = tree.wd))
t1   = subset(x1, tree.wd == 1)
t2   = subset(x1, tree.wd == 2)
t3   = subset(x1, tree.wd == 3)
t4   = subset(x1, tree.wd == 4)
tree.wd = cutree(w, 4)
tree.wd
rm(list = ls(all = TRUE))
graphics.off()
# load data
x <-  read.table("C:/Users/user/Desktop/多變量11101/uscrime.dat")
n1 <- nrow(x)
n2 <- ncol(x)
# standardize the data
# x <- scale(x)
x <- (x - matrix(mean(as.matrix(x)), n1, n2, byrow = T))/matrix(sqrt((n1 - 1) *
apply(x, 2, var)/n1), n1, n2, byrow = T)
# spectral decomposition
eig <- eigen((n1 - 1) * cov(x)/n1)
e <- eig$values
v <- eig$vectors
# eigenvalues and percentage
perc = e/sum(e)
cum  = cumsum(e)/sum(e)
xv   = as.matrix(x) %*% v # principal components
xv   = xv * (-1)
View(xv)
rm(list = ls(all = TRUE))
graphics.off()
# load data
x <-  read.table("C:/Users/user/Desktop/多變量11101/uscrime.dat")
n1 <- nrow(x)
# standardize the data
x <- scale(x)
# spectral decomposition
eig <- eigen((n1 - 1) * cov(x)/n1)
e <- eig$values
v <- eig$vectors
# eigenvalues and percentage
perc = e/sum(e)
cum  = cumsum(e)/sum(e)
xv   = as.matrix(x) %*% v # principal components
xv   = xv * (-1)
View(xv)
# correlation of the first 3 PC
corr = cor(x, xv)[, 1:3]
r12  = corr[1:11, 1:2]
r13  = cbind(corr[1:11, 1], corr[1:11, 3])
r32  = cbind(corr[1:11, 3], corr[1:11, 2])
r123 = corr[1:11, 1:3]
View(corr)
# clear variables and close windows
rm(list = ls(all = TRUE))
graphics.off()
# load data
data <- read.table("C:/Users/user/Desktop/多變量11101/uscrime.dat")
x <- data[,(3:9)]
# x <- cbind(x, rep("en"=x[1:9,], "mw"=x[10:21,], "sou"=x[22:37,], "wes"=x[38:50,]))
x1 <- scale(x)  # standardize variable
# define variable names
colnames(x1) = c("murder", "rape", "robbery", "assault", "burglary", "larcery", "autothieft")
# spectral decomposition
eig = eigen(cov(x1))
e = eig$values
v = eig$vectors[, 1:2]
dav = x1 %*% v
corr = cor(x1, -dav)[, 1:2]
View(corr)
# correlation coefficient
r <- round(cor(x1), digits = 3)
View(r)
library(ggplot2)
library(GGally)
# Scatterplot matrix for variables X1 to X7
ggpairs(data = as.data.frame(x1), alpha = 0.5)
